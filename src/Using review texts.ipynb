{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bw7WDyaFo3tD"
   },
   "source": [
    "This is the code for making a recommendation engine driven by the review text. The idea is to predict ratings for the business by understanding the user behaviour and the peculiar things offered by the business. For gauging the behaviour we use Natural Language Processing over the reviews. \n",
    "---\n",
    "\n",
    "\n",
    "(At each point we save the arrays and other objects to not lose them due to probable crashing of the sytem due to RAM exhaustion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e0Z7vBkWb7gt",
    "outputId": "56aa1aef-e91d-4af6-ee5a-0757f0e64cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26Y616DPoOjs"
   },
   "source": [
    "Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9ETGEmCK_w9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LikGkmBkoRwu"
   },
   "source": [
    "Setting up the path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ij4rniNdNlpP"
   },
   "outputs": [],
   "source": [
    "path1 = \"/content/drive/My Drive/Sample_trial/\"\n",
    "path = \"/content/drive/My Drive/yelp_dataset/\"\n",
    "path_data = \"/content/drive/My Drive/Datasets/\"\n",
    "city_name = \"Las Vegas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjXbYWoAoVMR"
   },
   "source": [
    "Generatting a list of unique business IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ix-Lyur5JDnk"
   },
   "outputs": [],
   "source": [
    "businesses = set()\n",
    "\n",
    "with open(path + 'business.json') as data_file:\n",
    "    for line in data_file:\n",
    "        buss = json.loads(line)\n",
    "        if buss['city'] == city_name:\n",
    "            businesses.add(buss['business_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdNIvO0GogLD"
   },
   "source": [
    "Counting the number of reviews given by each user and number of reviews recieved by a business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFiriR1HOSbA"
   },
   "outputs": [],
   "source": [
    "all_output = open(path + city_name + '_reviews.json', 'w')\n",
    "\n",
    "user_count = {}\n",
    "restaurant_count = {}\n",
    "\n",
    "with open(path+'review.json') as data_file:\n",
    "    for line in data_file:\n",
    "        review = json.loads(line)\n",
    "        if review['business_id'] in businesses:\n",
    "            all_output.write(line)\n",
    "            user_id, restaurant_id = review['user_id'], review['business_id']\n",
    "            if user_id not in user_count: user_count[user_id] = 0\n",
    "            user_count[user_id] += 1\n",
    "            if restaurant_id not in restaurant_count: restaurant_count[restaurant_id] = 0\n",
    "            restaurant_count[restaurant_id] += 1\n",
    "\n",
    "all_output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiaOsn9uosDm"
   },
   "source": [
    "Filtering out the users and businesses having less than 25 corresponding reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQOLzu3wO1XO"
   },
   "outputs": [],
   "source": [
    "active_user_threshold = 25\n",
    "eligible_users = [user_id for (user_id, count) in user_count.items() if count >= active_user_threshold]\n",
    "eligible_restaurants = [rest_id for (rest_id, count) in restaurant_count.items() if count >= active_user_threshold]\n",
    "print (len(eligible_users), 'users are eligible, out of a total of', len(user_count))\n",
    "print (len(eligible_restaurants), 'restaurants are eligible, out of a total of', len(restaurant_count))\n",
    "eligible_users = set(eligible_users)\n",
    "eligible_restaurants = set(eligible_restaurants)\n",
    "filter_output = open(path + city_name + '_reviews_filtered.json', 'w')\n",
    "total_count, eligible_review_count = 0, 0\n",
    "with open(path + city_name + '_reviews.json') as data_file:\n",
    "    for line in data_file:\n",
    "        total_count += 1\n",
    "        review = json.loads(line)\n",
    "        if review['user_id'] in eligible_users and review['business_id'] in eligible_restaurants:\n",
    "            filter_output.write(line)\n",
    "            eligible_review_count += 1\n",
    "\n",
    "print (eligible_review_count, 'reviews eligible, out of a total of', total_count)\n",
    "\n",
    "filter_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpP69eZhp9V-"
   },
   "source": [
    "Extracting the ratings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zq6qxgNsRxrU"
   },
   "outputs": [],
   "source": [
    "skinny_output = open(path + city_name + '_reviews_ratings_only.txt', 'w')\n",
    "with open(path + city_name + '_reviews_filtered.json') as data_file:\n",
    "    for line in data_file:\n",
    "        review = json.loads(line)\n",
    "        #print(review)\n",
    "        skinny_output.write(review['user_id'] + '\\t' + review['business_id'] + '\\t' + str(review['stars']) + '\\t' + str(review['date']) + '\\n')\n",
    "\n",
    "skinny_output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OwYmg5aqEYO"
   },
   "source": [
    "We create a mapping for each user_id and business_id to be able to use those as index for the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "kIT-onnyR-Eh",
    "outputId": "38fbac16-e940-49c4-8a8a-50d2674395e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique users: 8635\n",
      "unique restaurants: 10721\n"
     ]
    }
   ],
   "source": [
    "# constructing the mappings from user_id and restaurant_id into matrix indices\n",
    "unique_user_counter, unique_rest_counter = 0, 0\n",
    "user_index_map, rest_index_map = {}, {}\n",
    "\n",
    "with open(path + city_name + '_reviews_filtered.json') as data_file:\n",
    "    for line in data_file:\n",
    "        review = json.loads(line)\n",
    "        if review['user_id'] not in user_index_map.keys(): \n",
    "            user_index_map[review['user_id']] = unique_user_counter\n",
    "            unique_user_counter += 1\n",
    "        if review['business_id'] not in rest_index_map.keys():\n",
    "            rest_index_map[review['business_id']] = unique_rest_counter\n",
    "            unique_rest_counter += 1\n",
    "\n",
    "print ('unique users:', unique_user_counter)\n",
    "print ('unique restaurants:', unique_rest_counter)\n",
    "\n",
    "# save the index mappings \n",
    "with open(path + city_name + '_filtered_user_index.json', 'w') as user_idx_file:\n",
    "    json.dump(user_index_map, user_idx_file)\n",
    "with open(path + city_name + '_filtered_restaurants_index.json', 'w') as rest_idx_file:\n",
    "    json.dump(rest_index_map, rest_idx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "jM8rO1iWSiKe",
    "outputId": "74ae1df2-0fc3-4bc4-cda5-e87ce080f7c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 8635 | Number of items = 10721\n",
      "load the index mappings\n",
      "split and construct matrix\n"
     ]
    }
   ],
   "source": [
    "header = ['user_id', 'business_id', 'rating','date']\n",
    "df = pd.read_csv(path + city_name + '_reviews_ratings_only.txt', sep='\\t', names=header)\n",
    "\n",
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.business_id.unique().shape[0]\n",
    "total_count = len(df)\n",
    "print ('Number of users = ' + str(n_users) + ' | Number of items = ' + str(n_items))\n",
    "\n",
    "print ('load the index mappings')\n",
    "with open(path + city_name + '_filtered_user_index.json') as user_idx_file:\n",
    "    user_id_map = json.load(user_idx_file)\n",
    "with open(path + city_name + '_filtered_restaurants_index.json') as rest_idx_file:\n",
    "    item_id_map = json.load(rest_idx_file)\n",
    "\n",
    "print ('split and construct matrix')\n",
    "train_data, test_data = train_test_split(df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbg_HPUfIstx"
   },
   "outputs": [],
   "source": [
    "inverse_user_map = dict([(value, key) for key, value in user_id_map.items()]) \n",
    "inverse_item_map = dict([(value, key) for key, value in item_id_map.items()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUkwuaJsqWx_"
   },
   "source": [
    "Creating a ratings matrix and calculating the sparsity of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xqrxo2LXUBsN"
   },
   "outputs": [],
   "source": [
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "\n",
    "total_count = 0\n",
    "\n",
    "for line in train_data.itertuples():\n",
    "  train_data_matrix[user_id_map[line[1]], item_id_map[line[2]]] = line[3]\n",
    "  total_count += 1\n",
    "  \n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "  test_data_matrix[user_id_map[line[1]], item_id_map[line[2]]] = line[3]\n",
    "  total_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "vZR7ctPga_ju",
    "outputId": "a4b41b0c-5361-4af5-b1ea-deaf778ca397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total users: 8635\n",
      "total items: 10721\n",
      "total reviews: 456288\n",
      "sparsity: 0.004928802424520395\n"
     ]
    }
   ],
   "source": [
    "print ('total users:', n_users)\n",
    "print ('total items:', n_items)\n",
    "print ('total reviews:', total_count)\n",
    "print ('sparsity:', total_count / (n_users * n_items * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQSycad0XbRj"
   },
   "outputs": [],
   "source": [
    "# train_data_matrix = scipy.sparse.csr_matrix(train_data.values)\n",
    "# test_data_matrix = scipy.sparse.csr_matrix(test_data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyiByZDQqj0g"
   },
   "source": [
    "Concatenating reviews for each user and for each business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbY9sox4cAMc"
   },
   "outputs": [],
   "source": [
    "user_concat = ['' for _ in range(len(user_id_map))]\n",
    "restaurant_concat = ['' for _ in range(len(item_id_map))]\n",
    "\n",
    "with open(path + city_name + '_reviews_filtered.json') as data_file:\n",
    "\tfor line in data_file:\n",
    "\t    review = json.loads(line)\n",
    "\t    text, user_id, restaurant_id = review['text'], review['user_id'], review['business_id']\n",
    "\t    user_concat[user_id_map[user_id]] += ' ' + text\n",
    "\t    restaurant_concat[item_id_map[restaurant_id]] += ' ' + text\n",
    "\n",
    "with open(path + city_name + '_users_concat_reviews.txt', 'w') as data_file:\n",
    "\tjson.dump(user_concat, data_file)\n",
    "\n",
    "with open(path + city_name + '_restaurants_concat_reviews.txt', 'w') as data_file:\n",
    "\tjson.dump(restaurant_concat, data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSPHvV53qxcD"
   },
   "source": [
    "Creating a TF-IDF vector for each of the concatenated reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Qdx1T5-_djpd",
    "outputId": "dbb620bd-4101-4f64-9b5e-c0810f3021dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing tf-idf matrix for the restaurants\n",
      "constructing tf-idf matrix for the users\n"
     ]
    }
   ],
   "source": [
    "with open(path + city_name + '_users_concat_reviews.txt') as data_file:\n",
    "\tuser_corpus = json.load(data_file)\n",
    "\n",
    "with open(path + city_name + '_restaurants_concat_reviews.txt') as data_file:\n",
    "\trestaurant_corpus = json.load(data_file)\n",
    "\n",
    "# directly to tf-idf matrix\n",
    "vectorizer = TfidfVectorizer(min_df = 1, stop_words = 'english')\n",
    "\n",
    "print ('constructing tf-idf matrix for the restaurants')\n",
    "restaurants_X = vectorizer.fit_transform(restaurant_corpus)\n",
    "\n",
    "print ('constructing tf-idf matrix for the users')\n",
    "user_X = vectorizer.transform(user_corpus)\n",
    "\n",
    "with open(path + city_name + '_user_X.np', 'wb') as file:\n",
    "\tnp.save(file, user_X)\n",
    "\n",
    "with open(path + city_name + '_restaurants_X.np', 'wb') as file:\n",
    "\tnp.save(file, restaurants_X)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2K_AwkC92_p"
   },
   "outputs": [],
   "source": [
    "user_X = np.load(path + city_name + '_user_X.np',allow_pickle=True)\n",
    "restaurants_X = np.load(path + city_name + '_restaurants_X.np',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kuiPpHk4sYDm"
   },
   "source": [
    "Computing the similarities between vectors for user and vectors for restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7y39qNhDr6RF"
   },
   "outputs": [],
   "source": [
    "cosine_similarities = linear_kernel(user_X, restaurants_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xB8LyKqs7Zsp"
   },
   "outputs": [],
   "source": [
    "with open(path + city_name + '_cosine_similarities.np', 'wb') as file:\n",
    "\tnp.save(file, cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzLCdgdkhssM"
   },
   "outputs": [],
   "source": [
    "restaurants_X = np.load(path + city_name + '_restaurants_X.np',allow_pickle=True)\n",
    "restaurants_X = restaurants_X.item()\n",
    "train_matrix = train_data_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKEnSceSs2RF"
   },
   "source": [
    "Creating a user preference matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_b_SlOcnuKZ"
   },
   "outputs": [],
   "source": [
    "user_preference = np.zeros((train_matrix.shape[0], restaurants_X.shape[1]))\n",
    "for i in range(train_matrix.shape[0]):\n",
    "\tnonzero_index = np.nonzero(train_matrix[i,])[0]\n",
    "\tnonzero_values = train_matrix[i, nonzero_index]\n",
    "\tuser_preference[i,] = nonzero_values.dot(restaurants_X[nonzero_index,].toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qtf_Oakj8Zgl"
   },
   "outputs": [],
   "source": [
    "with open(path1 + city_name + '_user_preference.np', 'wb') as file:\n",
    "\tnp.save(file, user_preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxp49vwSKhan"
   },
   "outputs": [],
   "source": [
    "user_preference = np.load(path1 + city_name + '_user_preference.np',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gksyUV1Qsv0r"
   },
   "source": [
    "Normalizing the user preference matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyxxXIif-O2w"
   },
   "outputs": [],
   "source": [
    "user_preference = normalize(user_preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4AmEkD_-UxR"
   },
   "outputs": [],
   "source": [
    "with open(path1 + city_name + '_user_preference_normalize.np', 'wb') as file:\n",
    "\tnp.save(file, user_preference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcFs_k1ms69t"
   },
   "source": [
    "Cell to call the garbage cleaner explicitly to clear the RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ApWT7D-x-tLC",
    "outputId": "308a2a3f-7b10-4024-f845-954ef0632762"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4sFFzDUBCAC3"
   },
   "outputs": [],
   "source": [
    "user_preference = np.load(path1 + city_name + '_user_preference_normalize.np',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Vp6paUv4-gDs",
    "outputId": "7590916d-bca7-473c-f7a0-c155cb001ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing the similarity\n"
     ]
    }
   ],
   "source": [
    "print ('computing the similarity')\n",
    "cosine_similarities = linear_kernel(user_preference, restaurants_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TMqUaAEyCa6P"
   },
   "outputs": [],
   "source": [
    "with open(path + city_name + '_user_preference_cos.np', 'wb') as file:\n",
    "\tnp.save(file, cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryOO03CQqA3r"
   },
   "outputs": [],
   "source": [
    "overall_matrix = test_data_matrix + train_data_matrix\n",
    "\n",
    "# '_cosine_similarities.np' or '_user_preference_cos.np'\n",
    "sorting_method = '_cosine_similarities.np'\n",
    "\n",
    "cosine_similarities = np.load(path + city_name + sorting_method)\n",
    "\n",
    "test_predictions = np.zeros((n_users, n_items))\n",
    "train_predictions = np.zeros((n_users, n_items))\n",
    "\n",
    "overall_predictions = np.zeros((n_users,n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FyxcSaKptH_p"
   },
   "source": [
    "Making predictions for the user using weighted regression method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAFK9j5KDfUD"
   },
   "outputs": [],
   "source": [
    "for user_idx in range(n_users):\n",
    "\tprint ('making predictions for the user:', user_idx)\n",
    "\n",
    "\tlist_with_index = [(sim, rest_index) for (rest_index, sim) in enumerate(cosine_similarities[user_idx,])]\n",
    "\tlist_with_index.sort()\n",
    "\n",
    "\treverse_index = {old_index: new_index for (new_index, (_, old_index)) in enumerate(list_with_index)}\n",
    "\n",
    "\tnonzero_index = np.nonzero(train_data_matrix[user_idx,])[0]\n",
    "\n",
    "\tnonzero_values = train_data_matrix[user_idx, nonzero_index]\n",
    "\n",
    "\tsorted_index = [reverse_index[old_index] for old_index in nonzero_index]\n",
    "\t\n",
    "\tsorted_index = np.array(sorted_index).reshape(len(sorted_index), 1)\n",
    "\tnonzero_values = np.array(nonzero_values).reshape(len(nonzero_values), 1)\n",
    "\n",
    "\tclf = KernelRidge(alpha=1.0)\n",
    "\tclf.fit(sorted_index, nonzero_values)\n",
    "\n",
    "\t# make predictions\n",
    "\toverall_nonzero_index = np.nonzero(overall_matrix[user_idx,])[0]\n",
    "\tnew_index = [reverse_index[old_index] for old_index in overall_nonzero_index]\n",
    "\tnew_index = np.array(new_index).reshape(len(new_index), 1)\n",
    "\tpred = clf.predict(new_index)\n",
    "\tpred = np.maximum(1, np.minimum(pred, 5))\n",
    "\toverall_predictions[user_idx, overall_nonzero_index] = pred.reshape(1, len(overall_nonzero_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EloP9jSrtkX2"
   },
   "source": [
    "Function to calculate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Y4YeWRXD66h"
   },
   "outputs": [],
   "source": [
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "\n",
    "    return math.sqrt(mean_squared_error(prediction, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "HoIGCczuJLL8",
    "outputId": "ecb7ea15-27fd-4601-f3a3-359b44b6fc75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing rmse: 1.4088356224819985\n",
      "training rmse: 1.412949150924642\n"
     ]
    }
   ],
   "source": [
    "print ('testing rmse:', rmse(overall_predictions, test_data_matrix))\n",
    "print ('training rmse:', rmse(overall_predictions, train_data_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ar5dsFnCErPK"
   },
   "outputs": [],
   "source": [
    "with open(path + city_name + sorting_method + '_all_predictions.np', 'wb') as file:\n",
    "\tnp.save(file, overall_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SMSgkDUMEsd"
   },
   "outputs": [],
   "source": [
    "sorting_method = '_cosine_similarities.np'\n",
    "\n",
    "overall_predictions = np.load(path + city_name + sorting_method + '_all_predictions.np',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0grt7tf4u-Xe"
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(overall_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNf3vfivNa4O"
   },
   "outputs": [],
   "source": [
    "x.index = x.index.to_series().map(inverse_user_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttadlkFTNhPf"
   },
   "outputs": [],
   "source": [
    "x.columns = x.columns.to_series().map(inverse_item_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whhSG_EhvQD4"
   },
   "outputs": [],
   "source": [
    "x = x.unstack().reset_index(name='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gnpfwh5QJwe8"
   },
   "outputs": [],
   "source": [
    "x.to_csv(path + \"predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYt0Cj1Un7LG"
   },
   "source": [
    "# With last ratings as the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyP9QYA3N-WP"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(path_data + \"train_noE.csv\")\n",
    "test = pd.read_csv(path_data + \"test_noE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "20MKVrBymNGi"
   },
   "outputs": [],
   "source": [
    "train = train[[\"user_id\",\"business_id\",\"rating\"]]\n",
    "test = test[[\"user_id\",\"business_id\",\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4A53aqQ9nLyu"
   },
   "outputs": [],
   "source": [
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "\n",
    "total_count = 0\n",
    "\n",
    "for line in train.itertuples():\n",
    "  train_data_matrix[user_id_map[line[1]], item_id_map[line[2]]] = line[3]\n",
    "  total_count += 1\n",
    "  \n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test.itertuples():\n",
    "  test_data_matrix[user_id_map[line[1]], item_id_map[line[2]]] = line[3]\n",
    "  total_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HsJ8GauEnsWj"
   },
   "outputs": [],
   "source": [
    "restaurants_X = np.load(path + city_name + '_restaurants_X.np',allow_pickle=True)\n",
    "restaurants_X = restaurants_X.item()\n",
    "train_matrix = train_data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6R_yvjHoMov"
   },
   "outputs": [],
   "source": [
    "overall_matrix = test_data_matrix + train_data_matrix\n",
    "\n",
    "# '_cosine_similarities.np' or '_user_preference_cos.np'\n",
    "sorting_method = '_cosine_similarities.np'\n",
    "\n",
    "cosine_similarities = np.load(path + city_name + sorting_method)\n",
    "\n",
    "test_predictions = np.zeros((n_users, n_items))\n",
    "train_predictions = np.zeros((n_users, n_items))\n",
    "\n",
    "overall_predictions = np.zeros((n_users,n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAtTuOewoVfv"
   },
   "outputs": [],
   "source": [
    "for user_idx in range(n_users):\n",
    "\t#print ('making predictions for the user:', user_idx)\n",
    "\n",
    "\t# list of (similarity, real restaurant index), sorted by similarity\n",
    "\tlist_with_index = [(sim, rest_index) for (rest_index, sim) in enumerate(cosine_similarities[user_idx,])]\n",
    "\tlist_with_index.sort()\n",
    "\n",
    "\t# list of (real restaurant index, the index of that restaurant in the sorted similarity list),\n",
    "\t# this list is sorted by the real restaurant index\n",
    "\t# use this to convert form the old real restaurant index, into the new index in the list_with_index \n",
    "\treverse_index = {old_index: new_index for (new_index, (_, old_index)) in enumerate(list_with_index)}\n",
    "\t# reverse_index.sort()\n",
    "\n",
    "\t# the real index of non-zero-rating restaurant\n",
    "\tnonzero_index = np.nonzero(train_data_matrix[user_idx,])[0]\n",
    "\n",
    "\t# the ratings of this user, listed in the original order in the training matrix\n",
    "\tnonzero_values = train_data_matrix[user_idx, nonzero_index]\n",
    "\n",
    "\t# for each of the non-zero real restaurant index, the corresponding new index \n",
    "\tsorted_index = [reverse_index[old_index] for old_index in nonzero_index]\n",
    "\t\n",
    "\tsorted_index = np.array(sorted_index).reshape(len(sorted_index), 1)\n",
    "\tnonzero_values = np.array(nonzero_values).reshape(len(nonzero_values), 1)\n",
    "\n",
    "\tclf = KernelRidge(alpha=1.0)\n",
    "\tclf.fit(sorted_index, nonzero_values)\n",
    "\n",
    "\t# make predictions\n",
    "\toverall_nonzero_index = np.nonzero(overall_matrix[user_idx,])[0]\n",
    "\t#print (len(overall_nonzero_index))\n",
    "\tnew_index = [reverse_index[old_index] for old_index in overall_nonzero_index]\n",
    "\tnew_index = np.array(new_index).reshape(len(new_index), 1)\n",
    "\tpred = clf.predict(new_index)\n",
    "\tpred = np.maximum(1, np.minimum(pred, 5))\n",
    "\t#print (pred)\n",
    "\toverall_predictions[user_idx, overall_nonzero_index] = pred.reshape(1, len(overall_nonzero_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "G36bZ_aup3ZL",
    "outputId": "749ce7ed-d292-400a-9572-b5919cdbaa91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing rmse: 1.5558682235279286\n",
      "training rmse: 1.405905385400033\n"
     ]
    }
   ],
   "source": [
    "print ('testing rmse:', rmse(overall_predictions, test_data_matrix))\n",
    "print ('training rmse:', rmse(overall_predictions, train_data_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3CxCu73p3ZO"
   },
   "outputs": [],
   "source": [
    "with open(path_data + city_name + sorting_method + '_all_predictions.np', 'wb') as file:\n",
    "\tnp.save(file, overall_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N84enpUDp3ZQ"
   },
   "outputs": [],
   "source": [
    "sorting_method = '_cosine_similarities.np'\n",
    "\n",
    "overall_predictions = np.load(path_data + city_name + sorting_method + '_all_predictions.np',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNVhGTGzp3ZS"
   },
   "outputs": [],
   "source": [
    "predictions_reviews = pd.DataFrame(overall_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmpcBRVPp3ZX"
   },
   "outputs": [],
   "source": [
    "predictions_reviews.index = predictions_reviews.index.to_series().map(inverse_user_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOEp6ViYp3ZZ"
   },
   "outputs": [],
   "source": [
    "predictions_reviews.columns = predictions_reviews.columns.to_series().map(inverse_item_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5D8SCtzvp3Zd"
   },
   "outputs": [],
   "source": [
    "predictions_reviews = predictions_reviews.unstack().reset_index(name='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPPfgCOOp-zR"
   },
   "outputs": [],
   "source": [
    "predictions_reviews.columns = [\"business_id\",\"user_id\",\"rating_reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0X2MiKgFTDi"
   },
   "outputs": [],
   "source": [
    "predictions_reviews.to_csv(path_data + \"predictions_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wk1NsQhkaucR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "reco_text_review",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
