{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_based",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spyxyKNpe5i0",
        "colab_type": "text"
      },
      "source": [
        "**Image based Score Prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnI_cQUAld32",
        "colab_type": "text"
      },
      "source": [
        "**Installing and importing all the required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGhlhQ4PodBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install lshash3 --upgrade\n",
        "!pip install img2vec_pytorch\n",
        "!pip install numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_cpbDisWXyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import json\n",
        "import ast\n",
        "import numpy as np\n",
        "from img2vec_pytorch import Img2Vec\n",
        "from PIL import Image\n",
        "import os\n",
        "from lshash.lshash import LSHash\n",
        "import operator\n",
        "import math\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp8E4BU7kenZ",
        "colab_type": "code",
        "outputId": "bb16489f-e494-4311-e9c5-9c77867a54d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "#This file is a dictionary of dictionaries having userid as key, list of buisiness_ids and a list of images of each business as values for the business_ids \n",
        "s=\"\"\n",
        "with open('/content/drive/My Drive/Columbia Photos/dict1.txt') as f:\n",
        "  for line in f:\n",
        "        s=s+line\n",
        "\n",
        "#Converting json compatible format\n",
        "json_data = ast.literal_eval(s)\n",
        "\n",
        "s3=\"\"\n",
        "with open('/content/drive/My Drive/mainDataForAP.txt') as f:\n",
        "  for line in f:\n",
        "        s3=s3+line\n",
        "\n",
        "all_ratings = ast.literal_eval(s3)\n",
        "\n",
        "s4=\"\"\n",
        "with open('/content/drive/My Drive/businessIdListForAP.txt') as f:\n",
        "  for line in f:\n",
        "        s4=s4+line\n",
        "\n",
        "last_ratings= ast.literal_eval(s4)\n",
        "\n",
        "\n",
        "s1=[]\n",
        "#Getting all image IDs from our subset\n",
        "for k,v in json_data.items():\n",
        "    for k1,v1 in v.items():\n",
        "        s1=s1+v1\n",
        "\n",
        "#this dictionary is for the last business the user has rated (key is user id and the value is the business id of the last business he has rated)\n",
        "s2=\"\"\n",
        "with open('/content/drive/My Drive/Columbia Photos/businessIdList-2.txt') as f:\n",
        "    for line in f:\n",
        "        s2=s2+line\n",
        "        \n",
        "json_data1 = ast.literal_eval(s2)\n",
        "\n",
        "#business to image list dicitonary (Key is the business ID and the value is the list of images for this business) Made for ease of use\n",
        "business2imglist={}\n",
        "for k,v in json_data.items():\n",
        "    for k1,v1 in v.items():\n",
        "        business2imglist[k1]=v1\n",
        "\n",
        "#image to business dictionary, We can use this whenever we will need the business id to which an image belongs to\n",
        "img2business={}\n",
        "for k,v in json_data.items():\n",
        "    for k1,v1 in v.items():\n",
        "        for i in v1:\n",
        "            img2business[i]=k1\n",
        "\n",
        "#dictonary of user and all the businesses he rated leaving the last business\n",
        "user2businesslist={} \n",
        "for k,v in json_data.items():\n",
        "    li=[]\n",
        "    for k1,v1 in v.items():\n",
        "        if not json_data1[k]==k1:\n",
        "            li.append(k1)\n",
        "    user2businesslist[k]=li\n",
        "\n",
        "user2oldimages={}\n",
        "user2lastimgs={}\n",
        "\n",
        "for k,v in user2businesslist.items():\n",
        "    j=[]\n",
        "    for i in v:\n",
        "        j=j+business2imglist[i]\n",
        "    user2oldimages[k]=j\n",
        "    \n",
        "for k,v in json_data1.items():\n",
        "    user2lastimgs[k]=business2imglist[v]\n",
        "\n",
        "users=list(user2businesslist.keys())\n",
        "businesses=list(business2imglist.keys())\n",
        "\n",
        "#All of the dictionaries defined above are made for ease of use and to not to rquire traversal of a dictionary of dicitonaries each time"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwti5OnNnCOF",
        "colab_type": "text"
      },
      "source": [
        "**Getting the embeddings of all the images in the subset and saving them**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRH6S-nnrXD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The embeddings are stored in a dictionary with key as the image_id (which is also the file name of the image)\n",
        "img2vec = Img2Vec()\n",
        "\n",
        "emb={}\n",
        "for filename in os.listdir('/content/drive/My Drive/Columbia Photos/imgs/'):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "      img = Image.open('/content/drive/My Drive/Columbia Photos/imgs/'+filename)\n",
        "      emb[filename[:-4]]=img2vec.get_vec(img, tensor=False)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sApWq2sSr7y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the embeddings we learnt\n",
        "np.save('embeddings.npy', emb) \n",
        "\n",
        "#Saving the embeddings file to local drive\n",
        "files.download( \"embeddings.npy\" ) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiOi4FA2nmGs",
        "colab_type": "text"
      },
      "source": [
        "**Loading the saved embeddings (instead of having to re-run the embedding generator each time)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLQnmsXeqTs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#As generating the embeddings will take long we can now load the generated embeddings instead\n",
        "#loading the embeddings file from drive\n",
        "emb = np.load('/content/drive/My Drive/embeddings.npy',allow_pickle='TRUE').item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCwL14YBn07q",
        "colab_type": "text"
      },
      "source": [
        "**The following set of chunks get the predicted rating of the last business of the users based on image similarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHVOyA-_uoFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining both our hyper-parameters here\n",
        "k1= 6\n",
        "k2= 6\n",
        "#User's last busiess closest neighbours\n",
        "user_lb_cn={}\n",
        "c=0\n",
        "matches=[]\n",
        "lsh = LSHash(6, 8)\n",
        "for k,v in json_data1.items():\n",
        "  t2=[]\n",
        "\n",
        "  #getting the closest neighbours for the each image of the last business the user rated, using eucledian similarity\n",
        "  for n in user2lastimgs[k]:\n",
        "    t=[]\n",
        "    # response = lsh.query(emb[n].flatten())\n",
        "    # print(len(response))\n",
        "    for j in user2oldimages[k]:\n",
        "      dist=lsh.euclidean_dist(emb[n].flatten(),emb[j].flatten())\n",
        "      #Not including repeated images\n",
        "      if dist!=0:\n",
        "        t.append((img2business[j],dist))\n",
        "      \n",
        "      else:\n",
        "        #we're getting the images that are same in different businesses, for different businesses the exact same images are uploaded, this is noise in the dataset\n",
        "        matches.append((n,j))\n",
        "\n",
        "\n",
        "    #getting top k1(Hyper-parameter 1) closest businesses for this image\n",
        "    t1=sorted(t, key = lambda x: x[1])\n",
        "    t2=t2+t1[:k1]\n",
        "\n",
        "  #This part of the code is essentially grouping all the same businesses and taking the average of their similarity scores\n",
        "  #This is because the images of the last business the user has rated can be closely similar to more than one image of a same business the user has previously rated\n",
        "  d={}\n",
        "  t3=[]\n",
        "  for b in t2:\n",
        "    key,val = b\n",
        "    d.setdefault(key, []).append(val)\n",
        "  for name, values in d.items():\n",
        "    t3.append((name,sum(values)/len(values)))\n",
        "  #Taking the overall top k2(Hyper-parameter 2) closest for the overall last business the user has rated\n",
        "  user_lb_cn[k]=sorted(t3, key = lambda x: x[1])[:k2]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuU0jG7wCPoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_busi_ratings_all_embs={}\n",
        "def predict(user_id):\n",
        "  w=0\n",
        "  r=0\n",
        "  for i in user_lb_cn[user_id]:\n",
        "    w=w+(1/i[1])\n",
        "\n",
        "  for j in user_lb_cn[user_id]:\n",
        "    r=r+((1/j[1])*all_ratings[user_id][j[0]])/w\n",
        "\n",
        "  last_busi_ratings_all_embs[user_id]=r\n",
        "  return r\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wen1orXr8jx7",
        "colab_type": "code",
        "outputId": "11f68d77-db70-4665-c6bc-6c319fb3f5ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def rmse():\n",
        "  score=0\n",
        "  n=len(json_data1)\n",
        "  for k,v in json_data1.items():\n",
        "    score=score+math.pow((predict(k)-last_ratings[k]),2)\n",
        "  \n",
        "  return math.sqrt(score/n)\n",
        "\n",
        "val=rmse()\n",
        "print(\"RMSE on hyper-parameter1 as\",k1,\"and hyper-parameter2 as\",k2,\"is:\",val)"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE on hyper-parameter1 as 6 and hyper-parameter2 as 6 is: 1.4640925642508344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eor3c6JIoUjF",
        "colab_type": "text"
      },
      "source": [
        "**Modification 1:** <br>\n",
        "Instead of using multiple embedding for each business, after this modification we will be using one average embedding vector for each business instead. Please refer the accompanying report document for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omQxkRLJhqk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Average image embeddings for each business\n",
        "\n",
        "img2vec1 = Img2Vec()\n",
        "\n",
        "emb1={}\n",
        "\n",
        "for k,v in business2imglist.items():\n",
        "  tmp=[]\n",
        "  for h in v:\n",
        "    img = Image.open('/content/drive/My Drive/Columbia Photos/imgs/'+h+'.jpg') \n",
        "    embs=img2vec1.get_vec(img, tensor=False)\n",
        "    tmp.append(embs.flatten())\n",
        "  #Getting the mean embeddings of each business\n",
        "  emb1[k]=sum(tmp)/len(tmp)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu4xi-ZcwvpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the average embeddings for businesses\n",
        "np.save('embeddings1.npy', emb1) \n",
        "\n",
        "#Downloading the average embeddings for businesses to the local drive\n",
        "files.download( \"embeddings1.npy\" ) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK4VybVhpSK-",
        "colab_type": "text"
      },
      "source": [
        "**Loading the saved embedding for the new model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ClT4ACzeJou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading the embeddings file from drive\n",
        "emb1 = np.load('/content/drive/My Drive/embeddings1.npy',allow_pickle='TRUE').item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIn-GpdipdLk",
        "colab_type": "text"
      },
      "source": [
        "**The following set of chunks get the predicted ratings of the last business of the users based on our modified model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOVERUsHkJdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lsh1= LSHash(6,8)\n",
        "\n",
        "def business_item_item():\n",
        "  user_last_avg_amb_dict={}\n",
        "  for k,v in user2businesslist.items():\n",
        "    dist=[]\n",
        "    #distance between the last rated business and the other businesses users have rated\n",
        "    for d in v:\n",
        "      dist.append((d,(lsh1.euclidean_dist(emb1[d],emb1[json_data1[k]]))))\n",
        "    user_last_avg_amb_dict[k]=dist\n",
        "  return user_last_avg_amb_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78tmRo2kzs-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_busi_ratings={}\n",
        "def predict1(user_id,dictt):\n",
        "  w=0\n",
        "  r=0\n",
        "  for i in dictt[user_id]:\n",
        "    w=w+(1/i[1])\n",
        "\n",
        "  for j in dictt[user_id]:\n",
        "    r=r+((1/j[1])*all_ratings[user_id][j[0]])/w\n",
        "\n",
        "  last_busi_ratings[user_id]=r\n",
        "  return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n19EWlw0sqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmse1(dictt):\n",
        "  score=0\n",
        "  n=len(json_data1)\n",
        "  for k,v in json_data1.items():\n",
        "    score=score+math.pow((predict1(k,dictt)-last_ratings[k]),2)\n",
        "  \n",
        "  return math.sqrt(score/n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPWlnh1N1RkW",
        "colab_type": "code",
        "outputId": "4fb5eab4-16f6-4e29-b1d9-dceae6768329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dictionary=business_item_item()\n",
        "vb=rmse1(dictionary)\n",
        "print(\"RMSE after the modification is:\",vb)"
      ],
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE after the modification is: 1.4355458015229903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUVRyr5QpxTz",
        "colab_type": "text"
      },
      "source": [
        "We can see that after the modification, the model performed better than all the variants of the previous models. So now we will use this model to generate predictions for all the businesses for each user and fill the entire user business matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc_RY7_BIy1m",
        "colab_type": "text"
      },
      "source": [
        "Predicting scores for all the user-business pairs, (not only for the last business the user has rated) based on image similarity. Since it doesn't align with our objective statement to predict ratings for all businesses and all users, we will not be using this dictionary, but should there be a need to predict any other ratings other than the last business this can be used in any future objectives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3JAeNMRFo56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_score(uid,bid):\n",
        "  distt=[]\n",
        "  sc=0\n",
        "  w=0\n",
        "  r=0\n",
        "  for v in user2businesslist[uid]:\n",
        "    dis= lsh1.euclidean_dist(emb1[v],emb1[bid])\n",
        "    if dis==0:\n",
        "      continue\n",
        "      #print(v,bid)\n",
        "    else:\n",
        "      distt.append((v,dis))\n",
        "\n",
        "  for i in distt:\n",
        "    w=w+(1/i[1])\n",
        "\n",
        "  for j in distt:\n",
        "    r=r+((1/j[1])*all_ratings[uid][j[0]])/w\n",
        "\n",
        "  return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PEatafD_AH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This will predict the ratings for all the businesses not only the last business\n",
        "def business_item_item_forall():\n",
        "  lsh1= LSHash(6,8)\n",
        "  new_dict={}\n",
        "\n",
        "  for k2,v2 in user2businesslist.items():\n",
        "    new_dict[k2]={}\n",
        "\n",
        "  for u1 in users:\n",
        "    for b1 in businesses:\n",
        "      if b1 in user2businesslist[u1]:\n",
        "        new_dict[u1][b1]= all_ratings[u1][b1]\n",
        "      else:\n",
        "        new_dict[u1][b1]=-1\n",
        "\n",
        "  for u in users:\n",
        "    for b in businesses:\n",
        "      if new_dict[u][b]==-1: #Predicting scores for all the businesses the user has not rated\n",
        "        new_dict[u][b]= get_score(u,b)\n",
        "\n",
        "  return new_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvDhkgQ9EwYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_matrix=business_item_item_forall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z76U5t8BfcEa",
        "colab_type": "text"
      },
      "source": [
        "Making all the scores into a dataframe. This dataframe has businesses (business IDs) in the rows and the users (user IDs) in the columns, the value in the row, column combination will be the score our model predicted for that particular user and the business. Then we save it as a pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsZiIVfxfa_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dff=pd.DataFrame(full_matrix)\n",
        "#Saving Dataframe as pickle for any possible future use\n",
        "dff.to_pickle(\"/content/drive/My Drive/full_matrix.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFt15DScpp6t",
        "colab_type": "text"
      },
      "source": [
        "**Ensemble model: (ALS + Image based prediciton (Using average embeddings))**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viDHru6zq2cU",
        "colab_type": "text"
      },
      "source": [
        "Here we are using the predictions ALS gave for the last business the user has rated in conjunction with the predicitons our model gave. We are not however giving equal weights for the predictions for both of these models though. We will use the actual ratings the user has given to the last business and fit a regression model with one variable for the rating our model predicted and another variable for the rating ALS predicted. Then the co-efficients found by regression are used as weights given to the predicted scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2VMQIQu4A6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generating the dataframe of our predicted score and actual score for the user business pairs of the last rated business of the user\n",
        "\n",
        "lio=[]\n",
        "for u1,v1 in json_data1.items():\n",
        "  lio.append((u1,v1,last_busi_ratings[u1],last_ratings[u1]))\n",
        "\n",
        "df1=pd.DataFrame(lio, columns=['user_id', 'business_id','predicted_rating','actual_rating'])\n",
        "\n",
        "#Dataframe having the predictions from ALS for the test-set\n",
        "dfg=pd.read_csv(\"/content/drive/My Drive/ALSPredictions3.csv\")\n",
        "\n",
        "merged=pd.merge(df1, dfg, on=['user_id', 'business_id'], how='inner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTTuZ7Q1TdOg",
        "colab_type": "code",
        "outputId": "b9a45dc5-87f3-422a-c288-c0e89b9e93b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#Fitting Regression to get the weights to give to the ensemble model\n",
        "\n",
        "X = merged[[\"predicted_rating\",\"prediction\"]]\n",
        "\n",
        "y = merged[[\"rating\"]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "regressor = LinearRegression()  \n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "print(\"Learned intercept parameter\")\n",
        "#To retrieve the intercept:\n",
        "print(regressor.intercept_)\n",
        "\n",
        "print(\"\\nLearned regression paramters 1st one for image model and 2nd one for ALS\")\n",
        "#For retrieving the slope:\n",
        "print(regressor.coef_)\n",
        "\n",
        "#This is printing the RMSE for the predicitons on the regression test_set\n",
        "predictions_ensemble = regressor.predict(X_test)\n",
        "print ('testing rmse:', math.sqrt(mean_squared_error(predictions_ensemble, y_test)))\n",
        "\n",
        "#This is printing the RMSE we have achieved after using the ensmeble with the predicted parameters on the last business\n",
        "print(\"\\nRMSE After Using ensmble to predict the ratings of the last business\")\n",
        "predictions_last_ratings = regressor.predict(X)\n",
        "print ('Ensemble rmse:', math.sqrt(mean_squared_error(predictions_last_ratings, y)))"
      ],
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learned intercept parameter\n",
            "[2.43098674]\n",
            "\n",
            "Learned regression paramters 1st one for image model and 2nd one for ALS\n",
            "[[0.32550042 0.02885347]]\n",
            "testing rmse: 1.3031321716065227\n",
            "\n",
            "RMSE After Using ensmble to predict the ratings of the last business\n",
            "Ensemble rmse: 1.3308631140825242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3sLw3Cz71P_",
        "colab_type": "text"
      },
      "source": [
        "We can see that this ensemble model performed better than both the ALS baseline and the bias baseline. (The codes for the all the baselines are in a different notebook)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcIn9C8jcia-",
        "colab_type": "text"
      },
      "source": [
        "The following code generates dataframes (with columns actual rating, predicted rating) for each model we explored using images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA-Qi_W5cd41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generating dataframe for ensemble\n",
        "\n",
        "#Converting ndarray to a list\n",
        "lis=[]\n",
        "for i in predictions_last_ratings:\n",
        "  lis.append(i[0])\n",
        "\n",
        "#Getting original ratings as a list\n",
        "nlis=merged['rating'].values.tolist()\n",
        "\n",
        "actual_rating = np.array(y)\n",
        "df_ensemble = pd.DataFrame({'actual rating': nlis, 'predicted rating': lis}, columns=['actual rating', 'predicted rating'])\n",
        "\n",
        "#Generating dataframe for multiple embedings based method\n",
        "tmp_tup=[]\n",
        "for k,v in last_busi_ratings_all_embs.items():\n",
        "  #Appending true ratings and our predicted ratings\n",
        "  tmp_tup.append((last_ratings[k],last_busi_ratings_all_embs[k]))\n",
        "\n",
        "df_all_emb=pd.DataFrame(tmp_tup, columns=['actual_rating', 'predicted_rating'])\n",
        "\n",
        "#Generating dataframe for average embeddings based method\n",
        "df_avg_emb=df1[['actual_rating','predicted_rating']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rjdoeq6rQ3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ensemble.to_csv(\"/content/drive/My Drive/Columbia Photos/ensemble_values.csv\",index=False)\n",
        "df_all_emb.to_csv(\"/content/drive/My Drive/Columbia Photos/all_embs_values.csv\",index=False)\n",
        "df_avg_emb.to_csv(\"/content/drive/My Drive/Columbia Photos/avg_embs_values.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8TIMuy6y_Kr",
        "colab_type": "text"
      },
      "source": [
        "All the files used in this code can be viewed by clicking on the following link. https://drive.google.com/open?id=1Rc841hGS9v2ZMsM1BQSGAcoYLJqy5BuE"
      ]
    }
  ]
}